{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2, Morning Session 1, MD/MC workshop\n",
    "### Dr. Michael Shirts, CU Boulder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we understand the information we get out of simulations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up some modules.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up some typical data from a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential = np.loadtxt('potential.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(potential)\n",
    "plt.xlabel(\"Frame of Simulation\")\n",
    "plt.ylabel(\"Potential Energy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: What should I report as the average potential energy of this simulation?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the uncertainty in this average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting distributions of non-normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betaf = scipy.stats.beta(0.8,4)\n",
    "x = np.linspace(0,2)\n",
    "plt.plot(x,betaf.pdf(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10000\n",
    "samples = betaf.rvs(num)\n",
    "plt.hist(samples,bins=50,density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pause and do some calculations\n",
    "\n",
    "* What would you report as the mean and standard deviation of this distribution?\n",
    "* What might be a better way to report the behavior of this distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the error in the mean of samples from this distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_of_means(num,nrepeats):\n",
    "    repeats = np.zeros(nrepeats)\n",
    "    for i in range(nrepeats):\n",
    "        repeats[i] = np.mean(betaf.rvs(num))\n",
    "    return repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distribution_of_means(2,1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distribution_of_means(5,1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distribution_of_means(100,1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Is the \"standard\" formula good here??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining independent points: the autocorrelation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: this is only valid for a _stationary_ timeseries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas implements an autocorrelation function as a function of _lag_ ($\\tau$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 500\n",
    "panda_pot = pd.Series(potential[start:]-np.mean(potential[start:]))\n",
    "print(panda_pot.autocorr(3))\n",
    "print(panda_pot.autocorr(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming Exercises\n",
    "\n",
    " * Use the `autocorr` function to calculate the entire autocorrelation function for all $\\tau$.  \n",
    " * Bonus points: Compute the autocorrelation function from scratch and compare.\n",
    " * Can you use your estimate the lag time $\\tau$ at which the system becomes uncorrelated (i.e. the autocorrelation function) to show points spaced more than $\\tau$ are independent? \n",
    " * Note: the formulas assume that the average if the timeseries is zero, so you should subtract it off first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh = pd.plotting.autocorrelation_plot(panda_pot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When does the ACF become essentially zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's your guess? Post in the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting ACF to an exponential to estimate correlation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential(x, r): #function f(x, r) = e^(r*x)\n",
    "    #return np.e ** (r * x)\n",
    "    return np.exp(r*x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function calculates tau by using an exponential fit.\n",
    "#To do this it uses scipy.optimize curve_fit\n",
    "#curve_fit will optimize any function to fit given data\n",
    "\n",
    "def tau_calc(ac_data, function = exponential): #takes in data and a python function\n",
    "    x_data = np.arange(len(ac_data))\n",
    "    pars, cov = curve_fit(f=function, xdata=x_data, ydata=ac_data, p0=[0], bounds=(-np.inf, np.inf))\n",
    "    #curve fit returns an np.array of optimally fit paramters(pars) and their coverience(cov)\n",
    "    #pars in this case will return an optimized value of k to fit the dataset\n",
    "    return -1/pars #tau = -1/k\n",
    "# scipy curve_fit documentation: https://towardsdatascience.com/basic-curve-fitting-of-scientific-data-with-python-9592244a2509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = tau_calc(acf)\n",
    "print(tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $\\tau$ is the time it takes to go from 1 to $1/e$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.arange(100)\n",
    "plt.plot(x_data,acf[0:100], label='autocorrelation') \n",
    "plt.plot(exponential(x_data, -1/tau), label = 'tau_calc estimation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Key point*: If the data fits an exponential well, we can treat it as uncorrelated samples if they samples are $2\\tau$ apart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 500\n",
    "mean = np.mean(potential[start:])\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsamples = len(potential)-start\n",
    "stderr_mean = np.std(potential[start:])/np.sqrt(nsamples-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we use instead of `nsamples` above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean = {mean}, sderr_mean = {stderr_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are probably interested in is the average potential energy per molecule, not the total potential energy. There were 900 water molecules in the simulation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the heat of vaporization $H_{vap}$ by:\n",
    "    \n",
    "\\begin{eqnarray}\n",
    "H_{vap} &=& H_{gas}-H_{liquid} \\\\\n",
    "        &=& U_{gas} + PV_{gas} - (U_{liquid} + PV_{liquid}) \\\\ \n",
    "        &=& \\left(U_{gas} - U_{liquid}\\right) - P \\left(V_{gas}-V_{liquid}\\right) \\\\ \n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the kinetic energy of liquid and vapor is the same, then this is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "U_{pot,gas} - U_{pot,liquid} + P(V_{gas}- V_{liquid})\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume ideality of gas ($PV=nRT$), and zero internal energy (which is valid for rigid water, like TIP3P or SPC/E), then we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "       &=& - U_{pot,liquid} + P\\left( \\frac{nRT}{P} - V_{liquid}\\right) \\\\\n",
    "        &=& - U_{pot,liquid} + nRT - PV_{liquid} \\\\\n",
    "\\mathrm{Molar} &=& -\\frac{U_{pot,liquid}}{N} + RT - \\frac{PV_{liquid}}{N}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "What is the $H_{vap}$ of water predicted by this simulation? What is the uncertainty in the estimate? How does it compare to the experimental $H_{vap}$ of water is 40.7 kJ/mol?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated tool for equilibration detection and correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use scipy statistical tests to see if two parts of the distributions are within the uncertainties of each other.  NOTE: have to use uncorrelated samples, or it will erroneously say that they are NOT within uncertainties of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A higher T-statistic means more difference between the data sets\n",
    "\n",
    "A lower P-value indicates a low probability that the difference in means was by chance\n",
    "\n",
    "P-value < 0.05 suggest they are two different data sets! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:  Use this code to estimate better than \"eyeballing\" what fraction is equibrated  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "cut = 500\n",
    "stop = 1000\n",
    "per_ind_sample = 1  # the number of indices between independent samples\n",
    "# compare \n",
    "scipy.stats.ttest_ind(potential[start:cut:per_ind_sample],potential[cut:stop:per_ind_sample],equal_var=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pymbar.timeseries tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymbar import timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t0, g, Neff_max] = timeseries.detectEquilibration(potential)\n",
    "# t0 is the initial point detected as starting the stationary point.\n",
    "# g is the estimate of the correlation time (approximately 2tau)\n",
    "# Neff_max is an estimate of the _effective_ number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t0)\n",
    "print(g)\n",
    "print(Neff_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subsample the data, picking every $n$th particle so that all the points are statistically independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = timeseries.subsampleCorrelatedData(potential[500:],g=5.66)\n",
    "len(potential[500:])\n",
    "len(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping for complicated uncertainties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference in statistics between a **population** and a **sample**. The population is all the possible observations out there. For instance if I were an epidemiologist, this might be all the people in the U.S. or all the children in Massachusetts.\n",
    "\n",
    "\n",
    "<img width=\"304\" height=\"232\" alt=\"Image result for population versus sample\" src=\"http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_BiostatisticsBasics/Sampling3.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this can also be applied to measurements in the lab as well such as possible voltage values from this battery. If you ran an infinite number of experients, you would get an infinite number of measurements, but either in this case of the whole US population case: accessing the actual population is virtually impossible.\n",
    "\n",
    "<img width=\"304\" height=\"223\" alt=\"Image result for thermometer gif\" src=\"https://media.giphy.com/media/26FL3uMhARSAvIZZS/giphy.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a scientist, you only have access to a sample. Part of designing an experiment is choosing how big your sample should be.\n",
    "\n",
    "But a key problem is: if you change your sample, it could change your sample mean and sample variance significantly. So the question is, how can you understand the variation in the **population** by only looking at your **sample**.\n",
    "\n",
    "Let me repeat: the question, which the bootstrap is trying to answer, is to understand the variation in the  **population** by understanding the  variation at your **sample**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we were in the lab, and we were making a calibration curve, say absorbance against concentration using UV-vis measurements. You generate the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(xlim=[0,5],ylim=[0,100],npoints=20,noise=1.0,seed=None,noise_model=\"even\"):\n",
    "    '''\n",
    "    This function just generates some random data that might look like at UV-vis output.\n",
    "    Inputs: range of data in the x and y direction, the number of points, and the level of noise.\n",
    "    Add random number seed to allow repetability\n",
    "    \n",
    "    We have two types of noise; even noise, and the other exponential noise model.\n",
    "    '''\n",
    "    np.random.seed(seed) # so we can control the random noise\n",
    "    x = np.linspace(xlim[0], xlim[1], npoints)\n",
    "    y_raw = np.linspace(ylim[0], ylim[1], npoints)\n",
    "    if noise_model == \"even\":\n",
    "        y = y_raw + noise*np.random.rand(npoints)\n",
    "    else:\n",
    "        y = y_raw + np.exp(x*np.random.rand(npoints)) - 1\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0\n",
    "xmax = 5\n",
    "\n",
    "ymin = 0\n",
    "ymax = 100\n",
    "\n",
    "npoints = 50\n",
    "\n",
    "# in the absence of noise, the slope is 25 and the intercept is 0, \n",
    "# but because of the heteroscedasticity, the linear fit will be somewhat biased.\n",
    "\n",
    "x, y = generate_data(xlim=[xmin,xmax],ylim=[ymin,ymax],noise=1.0,npoints=npoints,noise_model=\"exponenential\", seed=1)\n",
    "\n",
    "\n",
    "results = stats.linregress(x,y)\n",
    "slope = results.slope\n",
    "intercept = results.intercept\n",
    "y_fit = intercept + np.array([xmin,xmax])*slope\n",
    "original_data = pd.DataFrame({'x': x, 'y':y})\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the data and the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.scatter(original_data['x'], original_data['y'], alpha=0.5, label='raw data')\n",
    "ax.plot(np.array([0,5]), y_fit, linewidth=4, label='linear fit')\n",
    "ax.set_xlim([0, 5])\n",
    "ax.set_ylim([0, 200])\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But uh oh. You can see that some are those points have error getting a little large. You want to get some indication of how your calibration curve would change if you repeated the experiment again. \n",
    "\n",
    "Instead of actually repeating the experiment, you decide to use bootstrapping, and *estimate the variation within your sample as a replacement for the variation within your population.*\n",
    "\n",
    "To do this, you will generate \"new\" samples, picking from the data **with replacement**, pretending the data you collected is **entirely representative** of the actual population) **with replacement** and perform a regression on this resampling of your sample.\n",
    "**With replacement**, means you can draw the same point multiple times, is an important statement. We are assuming our population is much, much, bigger than our sample, but has the exact same distribution as our sample.  So when we withdraw the value at has x=1.456, there are still infinte more points with x=1.456 remaining. So the next point you pick is just as likely to be x=1.456 as it was the first time you drew it.\n",
    "\n",
    "You will generate some code to do this. First, though, we will see what happens if we resample from the original population, then we will see what bootstrapping can do. \n",
    "\n",
    "So first, we are drawing from the _population_, because we are regenerating the entire dataset, including with new amounts of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_from_population(data,npoints=20,ndraws=10):\n",
    "    '''\n",
    "    Draw from the population using the generate_data function,\n",
    "    and determining the slope and intercept for each process. \n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    ax.scatter(data['x'], data['y'], alpha=0.5, label='raw data')\n",
    "\n",
    "    xmin = data['x'].min()\n",
    "    xmax = data['x'].max()\n",
    "    \n",
    "    # initializing outputs\n",
    "    slope = np.zeros(ndraws)\n",
    "    intercept = np.zeros(ndraws)\n",
    "\n",
    "    for i in range(ndraws):\n",
    "        # vvvv - these are the lines to change for bootstrap\n",
    "        #x, y = generate_data(xlim=[xmin,xmax],ylim=[ymin,ymax],npoints=npoints,noise=10.0) # don't set the seed, so we get new random data.\n",
    "        x, y = generate_data(xlim=[xmin,xmax],ylim=[ymin,ymax],npoints=npoints,noise=1.0,noise_model=\"exponenential\") # don't set the seed, so we get new random data.\n",
    "        sample = pd.DataFrame({'x': x, 'y':y})\n",
    "        # ^^^^ \n",
    "        results = stats.linregress(sample['x'].values,sample['y'])\n",
    "        slope[i] = results.slope\n",
    "        intercept[i] = results.intercept\n",
    "        y_fit = intercept[i] + np.array([0,5])*slope[i]\n",
    "        ax.plot(np.array([xmin,xmax]), y_fit, linewidth=2, color='b', alpha=0.2)\n",
    "    return slope, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_from_population(original_data,npoints=20,ndraws=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat this, like, 5000 times, generate a distribution of slopes and intercepts, and use the distributions to calculate confidence intervals on the slope and intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, intercepts = draw_from_population(original_data,npoints=50,ndraws=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(slopes,bins=30)\n",
    "plt.title('slope values')\n",
    "plt.figure()\n",
    "plt.hist(intercepts,bins=30)\n",
    "plt.title('intercept values')\n",
    "ci = 95\n",
    "\n",
    "print('mean slope: {:.2f} +/- {:.2f} ({:2d}% CI: {:.2f} - {:.2f})'.format(np.mean(slopes), \n",
    "                                                       np.std(slopes,ddof=1), ci,\n",
    "                                                       np.percentile(slopes, ((100-ci)/2)),\n",
    "                                                       np.percentile(slopes, ((100+ci)/2))))\n",
    "\n",
    "print('mean intercept: {:.2f} +/- {:.2f} ({:2d}% CI: {:.2f} - {:.2f})'.format(np.mean(intercepts), \n",
    "                                                       np.std(intercepts,ddof=1), ci,\n",
    "                                                       np.percentile(intercepts, ((100-ci)/2)),\n",
    "                                                       np.percentile(intercepts, ((100+ci)/2))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these values compare to the ones estimated by the standard linear estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = stats.linregress(original_data['x'],original_data['y'])\n",
    "print(f\"slope + standard error of slope = {results.slope:.2f} +/- {results.stderr:.2f}\")\n",
    "print(f\"intercept + standard error of intercept = {results.intercept:.2f} +/- {results.intercept_stderr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem statistically consistent!  Both the constant term and the x term are within the error bars; or equivalently, the values are well within the distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, to the bootstrap\n",
    "\n",
    "To pull a sample with replacement, we will take advantage of `pandas` sampling capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data.head()\n",
    "subsample = original_data.sample(n=original_data.x.count(), replace=True)\n",
    "print(subsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some samples occur more than once, and some don't appear at all! That is what with replacement means!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacking Time**: That code draws *one* bootstrap sample. Copy the `draw_from_population` function from above to create a new function `bootstrap_from_data` to plot the lines from _multiple bootstrap samples_ of the original dataset instead of multiple draws from the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_from_bootstrap(data,ndraws=500):\n",
    "    '''\n",
    "    Draw from the boostrap and determining the slope and intercept for each process. \n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that bootstrap means will not be exactly the same as the population means, as each draw from the population will be different, and if we determine the bootstrap means more precisely, they will converge to the value for that sample, which should be well WITHIN the distribution of populations means (it will be have the distribution of the population distribution of means).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the code working, then run the line below - you should hopefully get something similar to the draw from population!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_boot, intercepts_boot = draw_from_bootstrap(original_data,ndraws=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hacking time, Part 2**: plot the histograms of the distribution of slopes and intercepts of the population and the bootstrap samples and compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(slopes,bins=30,alpha=0.4,density=True,label='population')\n",
    "plt.hist(slopes_boot,bins=30,alpha=0.4,color='orange',density=True,label='bootstrap')\n",
    "plt.legend()\n",
    "plt.title('slope values')\n",
    "ci = 95\n",
    "print('mean slope (bootstrap): {:.2f} +/- {:.2f} ({:2d}% CI: {:.2f} - {:.2f})'.format(np.mean(slopes_boot), \n",
    "                                                       np.std(slopes_boot,ddof=1),ci,\n",
    "                                                       np.percentile(slopes_boot, ((100-ci)/2)),\n",
    "                                                       np.percentile(slopes_boot, ((100+ci)/2))))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(intercepts,bins=30,alpha=0.4,density=True,label='population')\n",
    "plt.hist(intercepts_boot,bins=30,alpha=0.4,color='orange',density=True,label='bootstrap')\n",
    "plt.title('intercept values')\n",
    "plt.legend()\n",
    "ci = 95\n",
    "print('mean intercept (bootstrap): {:.2f} +/- {:.2f} ({:2d}% CI: {:.2f} - {:.2f})'.format(np.mean(intercepts_boot), \n",
    "                                                       np.std(intercepts_boot,ddof=1),ci,\n",
    "                                                       np.percentile(intercepts_boot, ((100-ci)/2)),\n",
    "                                                       np.percentile(intercepts_boot, ((100+ci)/2))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping for complicated function distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping is generalizable to any statistical quantities you may be interested in. One useful example is error propagation. If you know the error in raw datasets, and you want to know how that error will impact downstream calculations, you can use bootstrapping to do it **without having to do the calculus** involved in standard error propagation, or if the error propagation is really impossible to do. \n",
    "\n",
    "For example, try obtaining the distribution of the log of the absolute value of the (intercept/slope) in the linear fit above. What is the average and the distribution of this quantity? (Why this quantity? No reason, really, except that it would be REALLY HARD to get the distribution of errors using any standard error propagation). Compare this to what one would get using bootstrapping.\n",
    "\n",
    "Similarly, bootstrapping can be used to estimate error in ANY fitting procedure extending beyond linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "vals = np.sin(np.log(np.fabs(slopes/intercepts)))\n",
    "vals_boot = np.sin(np.log(np.fabs(slopes_boot/intercepts_boot)))\n",
    "plt.hist(vals,bins=30,alpha=0.4, density=True,label='population')\n",
    "plt.hist(vals_boot,bins=30,alpha=0.4,density=True,color='orange',label='bootstrap')\n",
    "plt.title('f(slope/intercepts) values')\n",
    "plt.legend()\n",
    "ci = 95\n",
    "\n",
    "print('mean sin(log(abs(slope/intercept))): {:.2f} ({:2d}% CI: {:.2f} - {:.2f})'.format(np.mean(vals), ci,\n",
    "                                                       np.percentile(vals, ((100-ci)/2)),\n",
    "                                                       np.percentile(vals, ((100+ci)/2))))\n",
    "print('mean sin(log(abs(slope/intercept))): (Bootstrap): {:.2f} ({:2d}% CI: {:.2f} - {:.2f})'.format(np.mean(vals_boot), ci,\n",
    "                                                       np.percentile(vals_boot, ((100-ci)/2)),\n",
    "                                                       np.percentile(vals_boot, ((100+ci)/2))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
